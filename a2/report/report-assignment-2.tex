\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,
            bindingoffset=0.2in,
            left=1in,
            right=1in,
            top=1in,
            bottom=1in,
            footskip=.25in]{geometry}

%###############################################################################

%\input{~/layout/global_layout}


%###############################################################################

% packages begin

\usepackage[
  backend=biber,
  sortcites=true,
  style=alphabetic,
  eprint=true,
  backref=true
]{biblatex}
\addbibresource{bibliographie.bib}
\usepackage[acronym]{glossaries}
\usepackage{multirow}
\usepackage{euscript}[mathcal]
% e.g. \mathcal{A} for fancy letters in mathmode
\usepackage{amsmath,amssymb,amstext,amsthm}

\usepackage{mdframed}
\newmdtheoremenv[nobreak=true]{problem}{Problem}[subsection]
\newmdtheoremenv[nobreak=true]{claim}{Claim}[subsection]
\newtheorem{definition}{Definition}[subsection]
\newtheorem{lemma}{Lemma}[claim]
\newtheorem{plemma}{Lemma}[problem]

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{enumerate}
\usepackage[pdftex]{graphicx}
\usepackage{subcaption}
% 'draft' für schnelleres rendern mitübergeben -> [pdftex, draft]
% dadruch wird nicht das bild mitgerendered, sondern nur ein kasten mit bildname -> schont ressourcen

\usepackage{hyperref}

\usepackage{tikz}
\usetikzlibrary{arrows,automata,matrix,positioning,shapes}

% for adding non-formatted text to include source-code
\usepackage{listings}
\lstset{language=Python,basicstyle=\footnotesize}
% z.B.:
% \lstinputlisting{source_filename.py}
% \lstinputlisting[lanugage=Python, firstline=37, lastline=45]{source_filename.py}
%
% oder
%
% \begin{lstlisting}[frame=single]
% CODE HERE
%\end{lstlisting}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{wasysym}

\usepackage{titling}
\usepackage{titlesec}
\usepackage[nocheck]{fancyhdr}
\usepackage{lastpage}

\usepackage{kantlipsum}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

% packages end
%###############################################################################

\pretitle{% add some rules
  \begin{center}
    \LARGE\bfseries
} %, make the fonts bigger, make the title (only) bold
\posttitle{%
  \end{center}%
  %\vskip .75em plus .25em minus .25em% increase the vertical spacing a bit, make this particular glue stretchier
}
\predate{%
  \begin{center}
    \normalsize
}
\postdate{%
  \end{center}%
}

\titleformat*{\section}{\Large\bfseries}
\titleformat*{\subsection}{\large\bfseries}
\titleformat*{\subsubsection}{\normalsize\bfseries}

\titleformat*{\paragraph}{\Large\bfseries}
\titleformat*{\subparagraph}{\large\bfseries}

%###############################################################################

\pagestyle{fancy}
\fancyhf{}
% l=left, c=center, r=right; e=even_pagenumber, o=odd_pagenumber; h=header, f=footer
% example: [lh] -> left header, [lof,ref] -> fotter left when odd, right when even
%\fancyhf[lh]{}
%\fancyhf[ch]{}
%\fancyhf[rh]{}
%\fancyhf[lf]{}
\fancyhf[cf]{\footnotesize Page \thepage\ of \pageref*{LastPage}}
%\fancyhf[rf]{}
\renewcommand{\headrule}{} % removes horizontal header line

% Fotter options for first page

\fancypagestyle{firstpagestyle}{
  \renewcommand{\thedate}{\textmd{}} % removes horizontal header line
  \fancyhf{}
  \fancyhf[lh]{\ttfamily M.Sc. Computer Science\\KTH Royal Institute of Technology}
  \fancyhf[rh]{\ttfamily Period 2\\\today}
  \fancyfoot[C]{\footnotesize Page \thepage\ of \pageref*{LastPage}}
  \renewcommand{\headrule}{} % removes horizontal header line
}
%###############################################################################

\newcommand\extrafootertext[1]{%
    \bgroup
    \renewcommand\thefootnote{\fnsymbol{footnote}}%
    \renewcommand\thempfootnote{\fnsymbol{mpfootnote}}%
    \footnotetext[0]{#1}%
    \egroup
}

%###############################################################################

\title{
  \normalsize{DD2356 HT25 Applied}\\
  \normalsize{GPU Programming}\\
  \large{Assignment 2}
}
\author{
  \small Paul Mayer\textsuperscript{\textdagger}\\[-0.75ex]
%  \footnotesize\texttt{MN: }\\[-1ex]
  \scriptsize\texttt{pmayer@kth.se}
  \and
  \small Rishi Vijayvargiya\textsuperscript{\textdagger}\\[-0.75ex]
%  \footnotesize\texttt{MN: }\\[-1ex]
  \scriptsize\texttt{rishiv@kth.se}
}
\date{}


%###############################################################################
% define Commands

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\I}{\mathbb{I}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\renewcommand{\epsilon}{\varepsilon}


\newcommand{\itodo}[2][yellow]{\todo[inline, color=#1!40]{#2}}
\newcommand{\mtodo}[2][orange]{\todo[color=#1!40]{#2}}



%###############################################################################
\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother
%###############################################################################

\begin{document}
\maketitle
\extrafootertext{\textsuperscript{\textdagger}Authors made equal contribution to the project}
\thispagestyle{firstpagestyle}
\vspace{1em}

% content begin
%

\section*{Prefix}
The code for our project was submitted as a zip file. You can find code for the code-related questions of the assignment under \verb|q[x]/|, where \verb|x| is the question number. 

\tableofcontents
\newpage

\section{Exercise 1 -- Shared Memory and Atomics}
We will continuously refer to the Google Colab Notebook where we ran these experiments for this question using the Tesla T4 GPU. This notebook can be found under \verb|q1/colab.ipynb| in the submitted code. 

\subsection*{General Implementation}
Our GPU implementation is divided in to 2 functions: \verb|histogram_kernel| and \verb|convert_kernel|. Both kernels have the same threads per block: 1024, but differ in the number of thread-blocks. The former divides the input array into blocks, while the latter divides the number of bins into separate blocks. Thus, there are at least as many threads as input elements for \verb|histogram_kernel|, and there are \verb|NUM_BINS| threads for \verb|convert_kernel|.

The general idea for the \verb|histogram_kernel| is that each thread is responsible for one element in the input array. Similarly, for the \verb|convert_kernel|, each thread is responsible for the saturation of exactly one bin. We now answer the questions in the report.

\subsection{Q1: Optimizations Attempted}
\label{sec:q1_q1}
We attempted 3 optimizations, which can be found in functions \verb|histogram_kernel_v{x}| where \verb|x| ranges from 0, 1, and 2 in the \verb|q1/histogram.cu| file. We kept the \verb|convert_kernel| implementation the same. We will refer to each of these as optimizations 0, 1, and 2 respectively. 

When reporting the runtime results of these optimizations, we compare it to the CPU and to the other optimizations using input length $102,400,000$. All the timings mentioned below can be found in the Google Colab Notebook. While the Notebook includes h-to-d and d-to-h device transfer runtimes, we only refer to the kernel execution runtimes in this report when comparing the performance as that is what changes on switching kernel implementations. Note that the kernel execution times also include the execution time of the \verb|convert_kernel|. However, these execution times seem to be multiple order of magnitudes smaller than the execution time for \verb|histogram_kernel| (microsecond vs millisecond according to \verb|ncu| output), and thus are left in since they do not dominate.

\subsubsection{Optimization 0: Only Atomics}
Each thread is responsible for correctly allocating one element of the input array to its associated bin. This uses only the \verb|atomicAdd| function of CUDA.
 From just a timing perspective, this performed a lot better compared to the CPU version: $\sim12$ms on the GPU compared to  $\sim205$ms on the CPU for a uniform distribution. The times are in a similar range for a normal distribution. This makes it look like the increased contention added by the \verb|atomicAdd| is overruled by the benefit of simply parallelizing the computation on the GPU. 

\subsubsection{Optimization 1: Shared Mem + Atomics, One Thread Overworked}
We now use shared memory to keep a \textit{local bin} copy where we first create a local histogram per block, and then add up the local histograms of each block to the global histogram. In each block, the thread with index 0 initializes the local bin copy to 0. After initialization, each thread assigns the input element it is responsible for to the correct local bin using \verb|atomicAdd|. After all threads have done this, the thread with index 0 then adds each local bin's total to the corresponding global bin using \verb|atomicAdd|. The initialization and the \verb|atomicAdd| to the global bin are followed by a call to \verb|__syncthreads()|.

From a timing perspective, this optimization performs better than the CPU version: taking about $\sim154ms$ for a uniform distribution. However, it has the worst runtime compared to the other GPU optimizations. This is the case for both uniform and normal distributions. 

One likely reason for this could be that one thread being responsible for the initialization of 4096 bins causes the other threads to sit idle and wait because of the \verb|__syncthreads()| call. With a greater number of elements, there are a greater number of blocks and thus more threads waiting in this idle state. Similarly, only one thread accumulates the local bin counts to the global bins, which could further cause an imbalance across thread blocks because of the \verb|cudaDeviceSynchronize| call after the kernel execution -- where we wait for these overworked threads to finish their work.

\subsubsection{Optimization 2: Shared Mem + Atomics, Equal Thread Work}
This is very similar to Optimization 1, but differs in how we initialize the shared-memory local bins and accumulate their results. Each block has 1024 threads, and we have a constant 4096 bins. Thus, each thread is responsible for initializing and accumulating the results of 4 local bins. Thread 0 initializes and accumulates local bin 0, 1024, 2048, and 3072. Thread 1023 does the same for local bin 1023, 2047, 3071, and 4095. This way, all threads do a small amount of work and hopefully do not sit idle for long. We do some additional housekeeping around the atomic addition -- where we check that the global thread index is one for an element that actually exists. This is required because we want all threads to participate in the local bin initialization and accumulation, regardless of whether they have an input element they are responsible for. 

From a timing perspective, this optimization has the best performance: taking about $\sim 8ms$ for a uniform distribution. It also has the best performance for a normal distribution. 

\subsection{Q2: The Chosen Optimization}
From Section \ref{sec:q1_q1}, we believed that Optimization 2 should be the way to go because it had the best performance compared to the other optimization attempts. We further wanted to confirm this using \verb|ncu| profiling to compare Optimization 2 against the other optimizations. This table summarizes some of noteworthy results for $N = 102,400,000$ elements. This can be found in raw form in the Google Colab Notebook. 

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|l|l|c|c|c|}
\hline
\textbf{Distribution} 
    & \textbf{Optimization} 
    & \textbf{Duration} 
    & \textbf{Compute Throughput} 
    & \textbf{Achieved Occupancy} \\
\hline
\hline

\multirow{3}{*}{Uniform}
    & Opt 0 & 12.07ms & 11.03\% & 57.48\% \\ \cline{2-5}
    & Opt 1 & 153.5ms & 35.46\% & 16.24\% \\ \cline{2-5}
    & Opt 2 & 8.17ms & 50.2\% &  88.63\% \\
\hline
\hline

\multirow{3}{*}{Normal}
    & Opt 0 & 12.95ms & 9.53\% & 56.87\% \\ \cline{2-5}
    & Opt 1 & 153.5ms & 35.46\% & 16.24\% \\ \cline{2-5}
    & Opt 2 & 8.17ms & 50.26\% & 88.66\% \\
\hline

\end{tabular}}
\caption{Some \textit{ncu} Results Optimizations 0-2 of \textit{histogram\_kernel}}
\label{table:q1_q2}
\end{table}

Because of its lower duration, higher Compute Throughput, and high Achieved Occupancy -- we felt that our preliminary experiments were validated by \verb|ncu| and thus it was decided that Optimization 2 would be the one we would chose to go with. Note that our focus was only on \verb|histogram_kernel|, and the implementation of \verb|convert_kernel| was kept the same throughout. The following questions will be answered with Opt 2 being the implementation of \verb|histogram_kernel|.

\subsection{Q3: Global Memory Reads and Writes}
\begin{itemize}
\item \underline{In \textit{histogram\_kernel}}: The only time global memory is read from and written to is the \verb|atomicAdd| function, where we accumulate the local bins. Each thread performs 4 atomic additions to the global bin, and there are 1024 threads per block, which gives us 4096 global reads and writes per thread block (we assume \verb|atomicAdd| has 1 global read and 1 global write that are performed atomically). When there are $N$ input elements, there are $\lceil\frac{N}{1024}\rceil$thread blocks. So, in \verb|histogram_kernel|, there are $4096 \times \lceil\frac{N}{1024}\rceil$ global reads and writes in \verb|histogram_kernel| in one run.

\item \underline{In \textit{convert\_kernel}}: Each thread is associated with one global bin, and reads the value of the global bin to check if it is saturated and optionally overwrites the bin value with 127 if it is saturated. Thus, there are exactly 4096 reads and at most 4096 writes. Note that we cannot get the exact number of writes here since we do not know the input distribution. 
\end{itemize}

So, there are exactly $4096 \times (\lceil\frac{N}{1024}\rceil + 1)$ global reads in one run, and at most $4096 \times (\lceil\frac{N}{1024}\rceil + 1)$ global writes.

\subsection{Q4: Atomic Operations in One Run}
\begin{itemize}

\item \underline{In \textit{histogram\_kernel}}: Each thread does 4 atomic additions to add the accumulate the local bin to the global bin, and there are 1024 threads in one block. So, there are 4096 atomic operations per block to accumulate the local bin to the global bin. As stated earlier, there are $\lceil\frac{N}{1024}\rceil$ thread blocks. So, there are $4096 \times \lceil\frac{N}{1024}\rceil$ such atomic operations in \verb|histogram_kernel|. Additionally, each input element is the responsibility of one thread, where this thread atomically increments the thread block's local bin counter. So, there are a total of $N$ such atomic operations in \verb|histogram_kernel|. So, in total, there are $4096 \times \lceil\frac{N}{1024}\rceil + N$ atomic operations in \verb|histogram_kernel|

\item \underline{In \textit{convert\_kernel}}: There are no atomic operations in \verb|convert_kernel| (we assume by atomic operations for the question explicitly refer to those of the form \verb|atomicXXX| in the CUDA library)

\end{itemize}

So, there are a total of $4096 \times \lceil\frac{N}{1024}\rceil + N$ atomic operations in one run.

\subsection{Q5: Shared Memory Usage}

\begin{itemize}
\item \underline{In \textit{histogram\_kernel}}: Each block as an \verb|unsigned int| array of size 4096 to represent local bins. We assume \verb|sizeof(unsigned int)| is 4 bytes. So, each block uses $4096 \times 4 = 16384$ bytes of shared memory, or  16.384kB. For an input of size $N$, there are $\lceil\frac{N}{1024}\rceil$  blocks. Thus, in total, there is around $(16.384 \times \lceil\frac{N}{1024}\rceil)$ kB of shared memory used by \verb|histogram_kernel|. 

\item  \underline{In \textit{convert\_kernel}}: There is no shared memory used by \verb|convert_kernel|. 
\end{itemize}

Thus, in total around $(16.384 \times \lceil\frac{N}{1024}\rceil)$ kB of shared memory is used in one run.

\subsection{Q6: Impact of Value Distribution on Collision \& Contention}
We refer to the impact of distributions on \verb|histogram_kernel| only, as we do not think the amount of work \verb|convert_kernel| does changes from one distribution to the other (additionally, there are no atomic operations being used there). 

We would expect higher contention and collision in the normal distribution as opposed to a uniform distribution for the same number of input elements. This is because the distribution of values would be more concentrated at the bins near the mean (around bin 2047 in our case) and would decrease as we go away from the mean. In contrast, for a uniform distribution, we would expect an equal spread across all bins, which would mean that the contention for all bins would be roughly similar instead of being concentrated on a few. 

For a naive implementation which relies only on global memory (in our case, Opt 0), this would mean that a greater number of threads would be vying to atomically update these high frequency bins near the mean in a normally distributed set of input values. This would in turn cause worse runtimes as threads have to wait to make the updates for their elements. This is visible in Table \ref{table:q1_q2}, where for Opt 0, the runtimes and performance for a normal distribution are slightly worse than those for the uniform distribution for the same $N$. 

However, for implementations using shared memory and local bins, the contention is much less as fewer threads will be vying for the same bin (since there are at most 1024 threads in one block) even if the distribution is not uniform. Additionally, with the quicker access to local memory, the effects of contention are also likely to be less noticeable. The global memory accesses in Opt 1 and Opt 2 don't depend on the distribution, as they simply accumulate the values in local bins and this step would be the same for both uniform and normal input data sets. Thus, with shared memory, we would expect the distribution to have almost negligible impact on the performance. This is exactly what we see in Table \ref{table:q1_q2}, where for Opt 1 and Opt 2, the performance across distributions is almost identical.

\subsection{Q7: Histogram Plots}
For all the charts and all the profiling, we used 1024 threads per block. The total number of blocks for \verb|histogram_kernel| were $\lceil \frac{N}{1024} \rceil$ where $N$ is the number of input elements. For \verb|convert_kernel|, the number of blocks was $4096 / 1024 = 4$. 

The plots can be seen in Figure \ref{fig:q1_normal} and Figure \ref{fig:q1_uniform}. Note that these are the results after making sure that all bins saturate at 127. The raw \verb|.txt| files which contain the data plotted in the histogram can be found in the \verb|q1/| directory. The first lines of these files contain the number of bins (4096 in our case) and the subsequent lines contain the count for each bin. The result files were generated using \verb|generate_result.py| and the plots were generated using \verb|plot_histogram.py|. The Google Colab Notebook has the run which was used to generate the result text files. 

 \begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q1/normal_1024.png}
    \caption{With 1024 Elements}
\end{subfigure}
\hfill
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q1/normal_10240.png}
    \caption{With 10240 Elements}
\end{subfigure}
\hfill
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q1/normal_102400.png}
    \caption{With 102400 Elements}
\end{subfigure}
\hfill
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q1/normal_1024000.png}
    \caption{With 1024000 Elements}
\end{subfigure}
        
\caption{Histogram Plots for Normal Distribution}
\label{fig:q1_normal}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q1/uniform_1024.png}
    \caption{With 1024 Elements}
\end{subfigure}
\hfill
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q1/uniform_10240.png}
    \caption{With 10240 Elements}
\end{subfigure}
\hfill
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q1/uniform_102400.png}
    \caption{With 102400 Elements}
\end{subfigure}
\hfill
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q1/uniform_1024000.png}
    \caption{With 1024000 Elements}
\end{subfigure}
        
\caption{Histogram Plots for Uniform Distribution}
\label{fig:q1_uniform}
\end{figure}

\subsection{Q8: Profiling for N = 1024 and Additional Optimization}
The Google Colab Notebook contains the detailed information about the \verb|ncu| run for both uniform and normal distribution when $N = 1024$. 
\begin{itemize}
\item \underline{For Normal Distribution}: The Shared Memory Configuration Size was 32.77 kB and Achieved Occupancy was 91.35\% for \verb|histogram_kernel|. For \verb|convert_kernel|, Shared Memory Configuration Size was the same and Achieved Occupancy was 95.54\%.

\item \underline{For Uniform Distribution}: The Shared Memory Configuration Size was 32.77 kB and Achieved Occupancy was 92.77\% for \verb|histogram_kernel|. For \verb|convert_kernel|, Shared Memory Configuration Size was the same and Achieved Occupancy was 98.79\%.
\end{itemize}

The next optimization we would have liked to try was to change the bins that each thread in a block in \verb|histogram_kernel| is responsible for initializing and accumulating. Currently, we jump 1024 bins to get to the next bin that the thread is responsible for. Instead, we would like to experiment what would happen if we make each thread responsible for 4 \underline{consecutive} bin. Thus, instead of thread 0 being responsible for bin 0, 1024, 2048, and 3072 -- it would be responsible for bin 0, 1, 2, 3. This will reveal some information about access patterns on GPUs and which access patterns are better for shared memory.

\section{Exercise 2 -- Reduction}
In this task we implement a distributed reduction algorithm for a 1D vector of 32bit floats.
We start by implementing the initializing (which is performed on the CPU) and by providing a reference implemtation.
Then we implement a naive parallel version of the same algorithm.
From there we will iteratively improve and profile our solution.

\subsection{Initialization and Reference Solution}\label{sec:2_init}
We start by initializing a vector of variable size (given as input to the program) of uniformly distributed random values $r \in (0, 1)$.
For this we use a default seeded standard mersenne twister engine.
Our reference solution looks like this:
\begin{lstlisting}[language=c++, caption=Computation of Reference Solution, label=lst:a2_ref_sol]
static float compute_reference_solution(const float *a, const int input_length) {
    double reference_solution = 0.0;
    for (int i = 0; i < input_length; ++i) {
        reference_solution += static_cast<double>(a[i]);
    }

    return static_cast<float>(reference_solution);
}
\end{lstlisting}

Notably in \ref{lst:a2_ref_sol} is the use of static casts from floats to double.
This is necessary, because for large input sizes (close to the max sizes of signed integers) we can expect rounding or overflow errors.
We profiled the reference solution with both casting and without and did not observe any statistically noteble runtime deviations.
However, we did observe that it is necessary for ensuring a correct result.

\subsection{Na\"ive GPU implementation}\label{sec:2_naive}
The simplest solution we could come up with, is spawning $N$ threads, each adding $a[i]$ towars a shared result variable.
On the positive side: this approach is very simple, each thread just has to access the global index to which it corresponds, reads the value and add it towards the accumulator.
However, the main issue with this apprach is, that we have to ensure that the add and overwrite needs to be performed atomically.
Therefore, this approach not really parallelizes the computation, because only one thread at a time has access to the shared resource.
We implemented it anyway.
The reults are unsuprisingly bad:
\itodo{Add profiling and code}
Besides bad performance, we also run into precision errors for large inputs (see section \ref{sec:2_init})

\subsection{Block-level tiling}\label{sec:2_blt}
In section \ref{sec:2_naive}, identified multiple problems.
\begin{itemize}
  \item Blocked threads due to global \footnote{As far as we understood, CUDA is smart enough to identify on which level to block:
if the destination pointer is a shared variable (visible to threads in the same block), cuda synchronizes the threads within this block.
However, when the destination is in global memory, CUDA synchronizes access over all threads over all blocks.}
\verb|atomicAdd()|.
  \item Rounding problems due to adding small and big floats to together
\end{itemize}
We address both of these issues by introducing tiling.
\itodo{Write down idea}
\itodo{Profiling}
\itodo{Conclusion}

\subsection{Thread-level tiling}\label{sec:2_tlt}
By introducing tiling, we saw some performance improvement in section \ref{sec:2_blt}.
However, it is still below expectation in relation to the used level.
For this there are two possible explanations:
\begin{itemize}
  \item Still a lot of blocking behaviour (this time on block level)
  \item DRAM bursts not optimally used, e.g. because each thread independently accesses array.
\end{itemize}
Both problems could be addressed by introducing tiling on thread level.
Each thread does not add one element of the big array onto an accumulator, but independently accumulates $NT$ elements.
The thread can use a local register for this, which is a lot more efficient than shared or global memory.
Results from each thread are then accumulated on block level, and then globally.
Note that $NT$ is a paramter which can be optimised.
\itodo{Profiling}
\itodo{Conclusion}


\section{Exercise 3 -- Tiled Matrix Multiplication}
We will occasionally refer to the Google Colab Notebook where we ran some of the experiments for this question using the Tesla T4 GPU. This notebook can be found under \verb|q3/colab.ipynb| in the submitted code. We noticed that for most of the runs performed on the Tesla T4 GPU, the tiled matrix-multiplication runtimes seem to be greater than the naive GEMM implementation on the GPU. This was not the case on the H100 GPU (present on the school GPU cluster) -- where tiled matrix multiplication gave great speedups. 

We believe that this might be because of the Tesla T4 GPU is quite old compared to the H100 GPU, and thus lags behind when it comes to architectural advantages. For instance, the Tesla T4 GPU seems to have a lower L1/Shared Memory space, meaning that advantages of tiling might be limited compared to the H100 GPU. This, along with other architectural advances -- such as lower memory latency, better ability to deal with bank conflicts (by virtue of being a newer architecture), and higher L1/Shared-Memory, among other things -- is the likely reason that the performance difference is very apparent on the H100 GPU but does not manifest as well with the Tesla T4 GPU with our basic implementation of the tiled matrix-multiplication. 

Thus, for runtimes, we defer to using the H100 GPU available on the school's GPU cluster. However, for profiling, we will use the results for the Google Colab Tesla T4 GPU (since profiling tools are not available on the school's GPU cluster). The output for the runs requested in Q2 and Q3 on Google Colab can be found in the \verb|q3/colab.ipynb| file. For the Graph in Q5, we focus on the graph for the school cluster's GPU. The graph for the runtimes obtained on Google Colab can be found in the Appendix section.

\subsection{Q1: Tiled Global Mem Reads}
We will refer to \verb|tileX| and \verb|tileY| as $t_x$ and $t_y$ respectively. Additionally, we will assume that $A$ as dimensions $M \times K$ and $B$ has dimensions $K \times N$. 

In our implementation, we sweep through matrix $A$ in tile sizes of $t_x \times t_y$ and we sweep through the matrix $B$ in tile sizes of $t_y \times t_x$. Thus, each block of threads computes a $t_x \times t_x$ portion of the result matrix $C$, and in each implementation, we move the tiles forward by $t_y$ elements (this is explained in a bit more detail in the implementation of \verb|tiled_gemm| in the code). Thus, in each iteration of the sweep performed by a block of threads, there are $2 \times t_y \times t_x$ global memory reads. In total, there are $\frac{K}{t_y}$ such sweeps performed. Thus, there are a total of $2 \times t_y \times t_x \times \frac{K}{t_y}$ or $2Kt_x$ global reads per thread block. 

In total, our implementation creates $\frac{M}{t_y} \times \frac{N}{t_x}$ thread blocks. Thus, there are a total of $\frac{M \times N \times 2Kt_x}{t_yt_x}$ or $\frac{2MNK}{t_y}$ global memory reads performed by our \verb|tiled_gemm| implementation. In comparison to the naive matrix-multiplication, which performed $2MNK$ memory reads, there is a reduction factor of $t_y$ here, which tells us that the greater the size of the tile in the $y-$direction, the fewer the memory reads. This intuitively makes sense to us. 

Note that the presence of $t_y$ is there because of our specific implementation. We could have swept through $A$ and $B$ in the opposite way to get a reduction factor of $t_x$, what is important here is that a greater tile size leads to fewer global memory reads. In practice, we have usually seen that tile sweeps are in squares, so $t_y = t_x$. This is also the case for our 3 tile choices later on in the questions.


\subsection{Q2: Output for A = B = (1024 x 1024)}
The output screenshot for the given configuration can be seen in Figure \ref{fig:ex3_part2}. This was obtained on H100. The output matrix result has been truncated to only show the first 5 rows and columns, but an element-wise comparison was performed with the reference result, the results of which are printed after the truncated matrices. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{../images/q3/part2.png}
    \caption{Results for $A = B = (1024 \times 1024)$ on H100}
    \label{fig:ex3_part2}
\end{figure}

The results here show that the slowest computation was the one performed on the CPU. Then, the naive CUDA implementation was around 200x faster. Subsequently, each of the tiled implementations was even faster, with the biggest tile size -- $(32 \times 32)$ -- getting the fastest runtime. Note that we are only timing the CUDA kernels here (and in subsequent runtime images/plots), and the cost of memory transform to and from the device are omitted. 

This shows to us the obvious advantages of tiling and using the shared memory over the naive CUDA implementation. Additionally, this shoes that increasing the tile sizes reduces the amount of global memory reads by a greater factor, thereby reducing the effects of interacting with the slower global memory in performing the computation. 

\subsection{Q3: Output for A = (513 x 8192) and B = (8192 x 1023)} 

The output screenshot for the given configuration can be seen in Figure \ref{fig:ex3_part3}. This was also obtained on the H100 and follows the same output format as the previous configuration. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{../images/q3/part3.png}
    \caption{Results for $A = (513 \times 8192)$ and $B = (8192 \times 1023)$ on H100}
    \label{fig:ex3_part3}
\end{figure}

The absolute runtimes here are a bit larger, likely because of the increased dimensions. However, they seem to follow the same trend: the CUDA naive gemm kernel is a lot faster than the CPU implementation, but the tiled implementations are a lot faster too, decreasing the runtime as the tile sizes increase. Thus, a similar analysis as was performed for the previous configuration applies here. 

One caveat here is that the difference between the CUDA naive gemm and the tiled gemms, while still non-trivial, isn't as big as was the case with square matrices that would divide evenly in to the block sizes. This could be because there are more threads in rectangular matrix multiplication which could be out of bounds when performing the computation, which could mean that threads in the same warp need to follow different paths around \verb|if...else| statements, forcing their computation to be sequential. 

\subsection{Q4: Achieved Occupancy and Shared Mem Usage}
This information was obtained on the Tesla T4 GPU on Google Colab, and can be found in raw form in the \verb|q3/colab.ipynb| file.
\subsubsection{For A = B = (1024 x 1024)}
\begin{itemize}
\item \underline{For gemm}: Achieved Occupancy was \textbf{98.19\%} and Shared Memory Usage was \textbf{0 byte/block}. 
\item \underline{For tiled\_gemm ($8 \times 8$)}: Achieved Occupancy was \textbf{98.83\%} and Dynamic Shared Memory Usage was \textbf{1.02 Kbyte/block}. 
\item \underline{For tiled\_gemm ($16 \times 16$)}: Achieved Occupancy was \textbf{99.10\%} and Dynamic Shared Memory Usage was \textbf{4.10 Kbyte/block}. 
\item \underline{For tiled\_gemm ($32 \times 32$)}: Achieved Occupancy was \textbf{99.98\%} and Dynamic Shared Memory Usage was \textbf{16.38 Kbyte/block}. 
\end{itemize}

\subsubsection{For A = (513 x 8192) and B = (8192 x 1023)}
\begin{itemize}
\item \underline{For gemm}: Achieved Occupancy was \textbf{97.43\%} and Shared Memory Usage was \textbf{0 byte/block}. 
\item \underline{For tiled\_gemm ($8 \times 8$)}: Achieved Occupancy was \textbf{97.98\%} and Dynamic Shared Memory Usage was \textbf{1.02 Kbyte/block}. 
\item \underline{For tiled\_gemm ($16 \times 16$)}: Achieved Occupancy was \textbf{98.32\%} and Dynamic Shared Memory Usage was \textbf{4.10 Kbyte/block}. 
\item \underline{For tiled\_gemm ($32 \times 32$)}: Achieved Occupancy was \textbf{100\%} and Dynamic Shared Memory Usage was \textbf{16.38 Kbyte/block}. 
\end{itemize}

\subsection{Q5: Runtimes}
\label{sec:ex3_runtimes}
We use square matrices for multiplication, starting at $(16 \times 16)$ and going up to $(2048 \times 2048)$. The runtimes for the CPU version, the \verb|gemm| version, and the 3 \verb|tiled_gemm| version (with tile sizes of $8 \times 8$, $16 \times 16$, and $32 \times 32$ for each configuration were obtained 3 times and the average was plotted. The python script which automates this is the \verb|run_experiments.py| file in the \verb|q3/| directory. It produces a \verb|.txt| file called \verb|runtime_results.txt|. This can then be plotted using the \verb|plot.py| script, passing the name of the results file as the argument. 

The results plotted here can be found in their raw form in the \verb|school_times.txt| file, and the experiment was run on the H100 GPU available on the KTH cluster. Finally, recall that we only plot the times for the CUDA kernels, and do not include memory transfer times here (D-to-H or H-to-D). 

 \begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q3/h100_with_cpu.png}
    \caption{Runtimes With CPU}
    \label{fig:q3_ex5_with_cpu}
\end{subfigure}
\hfill
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q3/h100_no_cpu.png}
    \caption{Runtimes Without CPU}
    \label{fig:q3_ex5_without_cpu}
\end{subfigure}
        
\caption{Log-Log Plot of Different matmul Implementations on H100}
\end{figure}

We went with a log-log line graph instead of the bar-graph because the CPU runtimes were incredibly high and thus dwarfed the GPU kernel runtimes. Figure \ref{fig:q3_ex5_with_cpu} includes the runtime for the CPU, where it can be seen that it is faster than the naive CUDA kernel for smaller matrix sizes. However, as the matrix sizes increase, there is a much faster increase in the runtime of the CPU version, as opposed to the naive CUDA kernel \verb|gemm|, which remains somewhat constant in its runtime with an increase in the matrix dimensions. This could be because the overhead of launching the CUDA kernel and managing the threads dominates as opposed to the computation to be performed.

We also see that all the \verb|tiled_gemm| versions with different tile sizes always perform better than the naive CUDA \verb|gemm| implementation. This is likely because the advantages of using shared memory and lower calls to global memory. Additionally, for larger matrices, it seems to be the case that the higher tile sizes perform better than smaller tile sizes. This is because larger tiles read more data in to shared memory, which further decreases the amount of repeated global memory calls that need to be made. 

One interesting observation here is the rate of growth of the runtimes for the naive \verb|gemm| kernel and the tiled \verb|tiled_gemm| kerels. Figure \ref{fig:q3_ex5_without_cpu} shows the runtimes of just the different CUDA kernels. For \verb|gemm|, the rate of growth is incredibly slow -- being almost non-present for the first few matrix sizes, and then seeming to increase linearly near the last few bigger matrix sizes. However, the rate of growth of all the \verb|tiled_gemm| versions seems to follow a similar pattern to the CPU implementation albeit to a lesser extent. This could be because of the large number of global-memory calls present in the naive \verb|gemm| kernel -- with an increase in the number of elements, we also have an increase in the number of threads launched, but each thread accesses global memory a lot more frequently, which would mean that the runtime here is more dependent on the time it takes to access global memory than the actual computation being performed. However, in the tiled versions, the reduction of the global memory access means that the runtimes are more dependent on the amount of compute to be performed, which could explain the reason for the difference in the rate of runtime growth. 


\section{Appendix}
\subsection{Exercise 3 -- Runtimes from Google Colab}
Figure \ref{fig:appendix_t4_runtimes} shows the log-log line graphs of the runtimes obtained from the experiment in Section \ref{sec:ex3_runtimes} obtained on Google Colab on the Tesla T4 GPUs. As explained at the beginning of the report for Exercise 3, the different architectures of the H100 and the Tesla T4 GPU give rise to very different looking plots. The runtimes of the tiled versions and the naive CUDA version are a lot closer to each other for the intermediate-big matrix sizes than was the case on the H100 GPU. The biggest tile size -- $32 \times 32$ -- which saw the fastest runtimes on the H100 here does not start out-performing the other implementations until we get to the largest matrix multiplication. 

One of the reasons for this, among others, could be the fact that the lower L1/Shared-Memory space on the Tesla T4 GPU mean that if greater shared memory is used, then a greater number of cache misses occur, which could further inflate the runtime in other ways. To make up for these greater number of cache misses because of greater shared-memory usage thanks to greater tile sizes, perhaps a much greater number of computations need to be performed -- which seems to start being the case with $2048 \times 2048$ matrix multiplication. If we had Google Colab credits, we would have liked to investigate what happened on even bigger matrix sizes, but we did not want to run out of free usage time while there are still 2 assignments and a project left in the course. 
\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q3/t4_with_cpu.png}
    \caption{Runtimes With CPU}
    \label{fig:appendix_ex5_with_cpu}
\end{subfigure}
\hfill
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{../images/q3/t4_no_cpu.png}
    \caption{Runtimes Without CPU}
    \label{fig:appendix_ex5_without_cpu}
\end{subfigure}
        
\caption{Log-Log Plot of Different matmul Implementations on Tesla T4}
\label{fig:appendix_t4_runtimes}
\end{figure}

% content end
%###############################################################################

% \printbibliography

\end{document}
