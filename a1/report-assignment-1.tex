\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,
            bindingoffset=0.2in,
            left=1in,
            right=1in,
            top=1in,
            bottom=1in,
            footskip=.25in]{geometry}

%###############################################################################

%\input{~/layout/global_layout}


%###############################################################################

% packages begin

\usepackage[
  backend=biber,
  sortcites=true,
  style=alphabetic,
  eprint=true,
  backref=true
]{biblatex}
\addbibresource{bibliographie.bib}
\usepackage[acronym]{glossaries}

\usepackage{euscript}[mathcal]
% e.g. \mathcal{A} for fancy letters in mathmode
\usepackage{amsmath,amssymb,amstext,amsthm}

\usepackage{mdframed}
\newmdtheoremenv[nobreak=true]{problem}{Problem}[subsection]
\newmdtheoremenv[nobreak=true]{claim}{Claim}[subsection]
\newtheorem{definition}{Definition}[subsection]
\newtheorem{lemma}{Lemma}[claim]
\newtheorem{plemma}{Lemma}[problem]

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{enumerate}
\usepackage[pdftex]{graphicx}
\usepackage{subcaption}
% 'draft' für schnelleres rendern mitübergeben -> [pdftex, draft]
% dadruch wird nicht das bild mitgerendered, sondern nur ein kasten mit bildname -> schont ressourcen

\usepackage{hyperref}

\usepackage{tikz}
\usetikzlibrary{arrows,automata,matrix,positioning,shapes}

% for adding non-formatted text to include source-code
\usepackage{listings}
\lstset{language=Python,basicstyle=\footnotesize}
% z.B.:
% \lstinputlisting{source_filename.py}
% \lstinputlisting[lanugage=Python, firstline=37, lastline=45]{source_filename.py}
%
% oder
%
% \begin{lstlisting}[frame=single]
% CODE HERE
%\end{lstlisting}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{wasysym}

\usepackage{titling}
\usepackage{titlesec}
\usepackage[nocheck]{fancyhdr}
\usepackage{lastpage}

\usepackage{kantlipsum}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

% packages end
%###############################################################################

\pretitle{% add some rules
  \begin{center}
    \LARGE\bfseries
} %, make the fonts bigger, make the title (only) bold
\posttitle{%
  \end{center}%
  %\vskip .75em plus .25em minus .25em% increase the vertical spacing a bit, make this particular glue stretchier
}
\predate{%
  \begin{center}
    \normalsize
}
\postdate{%
  \end{center}%
}

\titleformat*{\section}{\Large\bfseries}
\titleformat*{\subsection}{\large\bfseries}
\titleformat*{\subsubsection}{\normalsize\bfseries}

\titleformat*{\paragraph}{\Large\bfseries}
\titleformat*{\subparagraph}{\large\bfseries}

%###############################################################################

\pagestyle{fancy}
\fancyhf{}
% l=left, c=center, r=right; e=even_pagenumber, o=odd_pagenumber; h=header, f=footer
% example: [lh] -> left header, [lof,ref] -> fotter left when odd, right when even
%\fancyhf[lh]{}
%\fancyhf[ch]{}
%\fancyhf[rh]{}
%\fancyhf[lf]{}
\fancyhf[cf]{\footnotesize Page \thepage\ of \pageref*{LastPage}}
%\fancyhf[rf]{}
\renewcommand{\headrule}{} % removes horizontal header line

% Fotter options for first page

\fancypagestyle{firstpagestyle}{
  \renewcommand{\thedate}{\textmd{}} % removes horizontal header line
  \fancyhf{}
  \fancyhf[lh]{\ttfamily M.Sc. Computer Science\\KTH Royal Institute of Technology}
  \fancyhf[rh]{\ttfamily Period 2\\\today}
  \fancyfoot[C]{\footnotesize Page \thepage\ of \pageref*{LastPage}}
  \renewcommand{\headrule}{} % removes horizontal header line
}
%###############################################################################

\newcommand\extrafootertext[1]{%
    \bgroup
    \renewcommand\thefootnote{\fnsymbol{footnote}}%
    \renewcommand\thempfootnote{\fnsymbol{mpfootnote}}%
    \footnotetext[0]{#1}%
    \egroup
}

%###############################################################################

\title{
  \normalsize{DD2356 HT25 Applied}\\
  \normalsize{GPU Programming}\\
  \large{Assignment 1}
}
\author{
  \small Paul Mayer\textsuperscript{\textdagger}\\[-0.75ex]
%  \footnotesize\texttt{MN: }\\[-1ex]
  \scriptsize\texttt{pmayer@kth.se}
  \and
  \small Rishi Vijayvargiya\textsuperscript{\textdagger}\\[-0.75ex]
%  \footnotesize\texttt{MN: }\\[-1ex]
  \scriptsize\texttt{rishiv@kth.se}
}
\date{}


%###############################################################################
% define Commands

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\I}{\mathbb{I}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\renewcommand{\epsilon}{\varepsilon}

%###############################################################################
\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother
%###############################################################################

\begin{document}
\maketitle
\extrafootertext{\textsuperscript{\textdagger}Authors made equal contribution to the project}
\thispagestyle{firstpagestyle}
\vspace{1em}

% content begin
%

\section*{Prefix}
The code for our project was submitted as a zip file. You can find code for the code-related questions of the assignment under \verb|q[x]/|, where \verb|x| is the question number. 

\tableofcontents
\newpage

\section{Exercise 1 - Reflection on GPU Computing}
Whereas modern CPUs are designed to reduce the latency in which computing problems are solved, often by using complex  branch prediction techniques and control flow optimisations, GPUs are designed to optimise for throughput.
Their architectual design accomodates this difference in design goal.

First, GPUs have significantly higher number of processing cores, resulting in a significantly higher theroretical number of peak FLOP/s.
Second, GPUs contain less and simpler control logic, which follows the SIMD/SIMT model.
The same operation can be performed on a large amount of data simulatneously, however, if the type of computation depends on the data itself (control flow oriented), GPUs can't make use of their highly parallel layout.
Lastly, GPUs have a lot less cache (per core). They rely on more coalesced memory access patterns, whereas CPUs can work more efficient when faced with random access patterns. All of this makes computations more throughput oriented.

Out of the top 10 supercomputers in the world, 9 use GPUs. The only outlier is the japanese supercomputer Fugaku which uses ARM-based A64FX processor instead.

\begin{table}
\parbox{.45\linewidth}{
\centering
\begin{tabular}{|l|}\hline
  \textbf{AMD}\\\hline
  \#1 El Capitan - AMD Instinct MI300A \\
  \#2 Frontier - AMD Instinct MI250X   \\
  \#6 HPC6 - AMD Instinct MI250X       \\
  \#9 LUMI - AMD Instinct MI250X       \\\hline
\end{tabular}
\caption{AMD Gpus}
}
\hfill
\parbox{.45\linewidth}{
\centering
\begin{tabular}{|l|}\hline
  \textbf{Nvidia}\\\hline
  \#4 JUPITER Booster - NVIDIA GH200 \\
  \#5 Eagle - NVIDIA H100 \\
  \#8 Alps - NVIDIA GH200 \\
  \#10 Leonardo - NVIDIA A100 SXM4 \\\hline
\end{tabular}
\caption{NVidia GPUs}
}
\hfill\\[1em]
\centering
\parbox{.45\linewidth}{
\centering
\begin{tabular}{|l|}\hline
  \textbf{Other}\\\hline
  \#3 Aurora - Intel Data Center GPU Max Series\\
  \#7 Fugaku - No GPUs\\\hline
\end{tabular}
\caption{Other}
}
\end{table}

The energy efficiency is depicted in table \ref{tab:ee}. Out of the top10, the european Alps computer is the most efficient.
\begin{table}
  \centering
  \begin{tabular}{|l|l|}\hline
  \textbf{Computer}&\textbf{Energy Efficiency (GFlops/watts)}\\\hline
  \#1 El Capitan       & 58.89 \\ 
  \#2 Frontier         & 54.98 \\
  \#3 Aurora           & 26.15 \\
  \#4 JUPITER Boooster & 60.62 \\
  \#5 Eagle            & No power consuption declared\\
  \#6 HPC6             & 56.48 \\
  \#7 Fugaku           & 14.78 \\
  \#8 Alps             & 61.04 \\
  \#9 LUMI             & 53.43 \\
  \#10 Leonardo        & 32.19 \\\hline
  \end{tabular}
  \caption{Energy Eddiciency of the top 10 based on top500.org}
  \label{tab:ee}
\end{table}
\section{Exercise 2 - First CUDA Program and GPU Metrics}
\label{sec:q2}
\subsection{Commented Code}
The required comments have been placed in the appropriate places in the attached code in the file \verb|vecAddKernel.cu|

\subsection{FLOPs and Reads}
There is exactly one FLOP to compute the value of each element of the final result array. Since the final array has $N$ elements, there are a total of $N$ FLOPs for vectors of length $N$. There are 2 memory reads used to compute each element of the final array (one read each to read the value of each of the operands in the addition). Again, there are a total of $N$ elements in the final array, meaning we have $2N$ global reads. 

Thus, there are \underline{$N$ FLOPs} and \underline{$2N$ global reads} in our addition kernel.

\subsection{CUDA Config for $N = 512$}
We keep a constant 1024 threads per block and aim to assign one element of the resulting array to one thread. Since there are $N = 512 < 1024$ elements involved here, we launch exactly \underline{1 thread block} and a total of \underline{1024 threads}.

\subsection{Achieved Occupancy for $N = 512$}
On the Tesla T4 on Google Colab, our kernel achieved an occupancy of \underline{$65.22\%$}. This can be seen in the \verb|colab_notebook.ipynb| file under the \verb|q2/| directory of the submitted code.

\subsection{Increasing Vector Length to $N = 263149$}
For us, our program seemed to work correctly when we increased our vector length to this number. 

\subsection{CUDA Config for $N = 263149$}
As can be seen in the \verb|ncu| output for $N = 263149$ in the \verb|colab_notebook.ipynb| file, we launch a total of \underline{257 blocks} with 1024 threads per block, giving us a total of \underline{263168 threads}. Note that there are 19 more threads than there are elements, which means that some threads do not do any work, but this is much lower than the 512 threads not doing any work for $N = 512$

\subsection{Achieved Occupancy for $N = 263149$}
This time, the achieved occupancy was \underline{90.25\%}. This can also be seen in the \verb|q2/colab_notebook.ipynb| file, and was computed on the Tesla T4 on Google Colab.

\subsection{Stacked Bar-Chart}
Runtimes plotted in Figure \ref{fig:q2_bar} were obtained on the Tesla T4 on Google Colab. We varied grid sizes from $2^{19}$ to $2^{28}$ (inclusive). The CPU timer as described in the tutorial on Canvas was used to wrap the data-transfer (HtoD and DtoH) and kernel execution code. A python script was used to run the experiment 3 times for each vector length and then finally the average of these runs were obtained and plotted. The python script for running the experiments can be found in \verb|q2/run_experiments.py|. The runtimes plotted were those obtained from the notebook \verb|q2/colab_notebook.py|. The raw numbers for the plot in Figure \ref{fig:q2_bar} can be seen in the file \verb|q2/runtime_results.txt|.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{images/q2_bar.png}
  \caption{Stacked Bar-Chart of Runtimes}
  \label{fig:q2_bar}
\end{figure}

As can be seen from Figure \ref{fig:q2_bar}, most of the time from the 3 pieces of code plotted (HtoD data transfer, DtoH data transfer, and kernel execution) is spent in data transfer (HtoD + DtoH). The kernel execution time, even for vectors as big as $2^{28}$ in size, is incredibly minimal compared to the data transfer times. This indicates that our kernel is \underline{memory-bound} as opposed to compute-bound. Additionally, as expected, the amount of time spent in the data transfers increases as the vector sizes increase. The kernel execution time increases as well, but that is not very evident as it is dwarfed by the data-transfer times. 

\section{Exercise 3 - 2D Dense Matrix Multiply}

\subsection{Kernel FLOPs}
We will use $A_r, A_c, B_r, B_c$ to denote \textit{numARows, numAColumns, numBRows, numBColumns} respectively. 
For the matrix multiplication $A \times B$, it must be the case that $A_c = B_r$. So, we have 3 numbers: $A_r, A_c, B_c$. 

To compute each element of the result matrix $C$, there are a total of $2\times A_c$ FLOPs performed. Computation of each element of $C$ requires 1 multiplication operation of the corresponding elements of $A$ and $B$, and 1 addition operation (to accumulate this product as a running total). These 2 FLOPs are thus performed a total of $A_c$ times, giving us $2A_c$ FLOPs for each element of $C$. There are a total of $A_r \times B_c$ elements in $C$, and thus a total of $2A_c \times A_rB_c$ = \underline{$2A_cA_rB_c$ FLOPs}.

\subsection{Memory Reads}
We will use the same notation as above. There are a total of $2\times A_c$ memory reads (1 read for the element of $A$ and one read for the element of $B$ -- and there are $A_c$ such products computed) to compute the product that is then accumulated in the rolling partial sum used to compute each element of $C$. This partial sum is accumulated in a temporary variable, so we do not count that as a read from \textit{global} memory. 

There are a total of $A_rB_c$ elements in $C$. Thus, the total global memory reads in our kernel is: \underline{$2A_c \times A_rB_c = 2A_cA_rB_c$}. 


\subsection{CUDA Config and Achieved Occupancy for $(127 \times 256) \times (256 \times 32)$}
This code was executed on the Tesla T4 GPU on Google Colab. The notebook \verb|q3/colab_notebook.ipynb| contains information about this and subsequent executions and profiling. 
For this configuration, a total of \underline{4096 threads} were launched, and a total of \underline{4 thread blocks} were created (all thread blocks were in the $x$ direction). Each thread block has 32 threads in the $x$ direction and 32 threads in the $y$ direction.

The Achieved Occupancy for this configuration according to \verb|ncu| was \underline{$96.62\%$}.

\subsection{Metrics for $(1024 \times 8191) \times (8191 \times 8197)$}
For us, the program still seemed to work when we tested it using this configuration. 

Profiling information and metrics for the run with this configuration can also be found in the \verb|q3/colab_notebook.ipynb| file. For this config, a total of \underline{8224 thread blocks} were created (32 in the $x$ direction, 257 in the $y$ direction) and a total of \underline{8,421,376 threads} were launched (for each thread block, there were 32 threads in the $x$ direction and 32 threads in the $y$ direction). The threads in each thread block were kept at a constant $32 \times 32$. The number of thread blocks in each direction depend on the number of rows and columns in the resulting matrix. For instance, the number of thread blocks in the $x$ direction can be computed as $(1024 + 32 - 1) / 32$ (where 32 is the number of threads). The same thing goes for the thread blocks in $y$ direction.

In our kernel, each thread is responsible for one element of the resulting matrix. There are a total of $1024 \times 8197 = 8,393,728$ elements in the resultant matrix. However, the number of threads launched are $8,421,376 > 8,393,728$. So, only \underline{$8,393,728$ threads} actually perform the computation in our kernel.

According to \verb|ncu| on Google Colab, we see an achieved occupancy of \underline{$98.23\%$}. 

\subsection{Stacked Bar Chart Using Doubles}
\label{sec:q3_stacked_bar_double}
The timings plotted were obtained on the Tesla T4 on Google Colab. We ran the matrix multiplication 3 times for each of these matrix sizes, and then plotted the average runtimes for each of the operations we were tracking. In the graph, the labels on the $x$-axis are of the format: $(A_r, A_c, B_r, B_c)$, all separated by spaces. The python script which simulates the experiment is \verb|q3/run_experiments.py| and the script used to plot the resulting runtimes is \verb|q3/plot.py|. The colab notebook in the \verb|q3/| directory contains the execution of the runs that were plotted, and the raw runtimes for this plot can be found in the \verb|q3/runtime_results_vecMatMultiply.txt| file. 

Using \verb|double| as the data type for the operands and the final resulting matrix, we observe the stacked bar chart in Figure \ref{fig:q3_bar_double}. We see that as the size of the matrix increases, the corresponding times for each of the operations tracked (HtoD, DtoH, and kernel execution) also seems to increase. However, the proportion of time allocated to the execution of the kernel seems to be a lot more when compared to the data transfer times in either direction. This indicates to us that this computation in the kernel seems to be more \underline{compute bound} than memory bound. Additionally, comparing this to the stacked bar chart from Figure \ref{fig:q2_bar}, we also see a stark difference in the kernel execution times. This is likely because of the fact that matrix multiplication involves a lot more FLOPs when compared to simple vector addition. Note that the DtoH transfer times seem to be the same for all the runs plotted because the resulting matrix in all cases has the same dimensions ($1200 \times 9000$).

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{images/q3_bar_double.png}
  \caption{Stacked Bar-Chart of Runtimes (using \textit{double})}
  \label{fig:q3_bar_double}
\end{figure}

\subsection{Stacked Bar Chart Using Float}
The setup for this experiment was exactly the same as for Section \ref{sec:q3_stacked_bar_double}, except we replaced all \verb|double| data-types of operands and the resulting matrix with \verb|float|. The raw runtimes plotted for this plot can be found in \verb|q3/runtime_results_vecMatMultiplyFloat.txt| file. 

Using \verb|float| as the data type for the operands and the final result, we observe the stacked bar chart in Figure \ref{fig:q3_bar_float}. This seems to follow a similar trend to the chart in Figure \ref{fig:q3_bar_double} -- where increasing the problem size seems to increase the transfer times and the kernel executions. However, the overall execution for each of the 3 tracked components seems to be shorter when compared to the corresponding measurements when using \verb|double|. This is likely because a \verb|float| has lower precision than \verb|double| and is thus more faster to compute with and to transfer from device to host (and vice-versa). The proportion of time spent in kernel execution is still considerably large though, especially compared to vector-addition which was examined in Section \ref{sec:q2}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{images/q3_bar_float.png}
  \caption{Stacked Bar-Chart of Runtimes (using \textit{float})}
  \label{fig:q3_bar_float}
\end{figure}

\section{Exercise 4 - Rodinia Benchmarks}
We used the \verb|hotspot3D| and the \verb|lavaMD|.

\subsection{Changes to Makefile}
\subsubsection{hotspot3D}
For \verb|hotspot3D|, the OpenMP Makefile was fine as is. For Cuda, the \verb|include| statement in the Makefile had to be changed to refer to the correct path for \verb|make.config|. The original line was: 
\begin{verbatim}
include ~/rodinia_3.0/common/make.config
\end{verbatim}
which was then changed to 
\begin{verbatim}
include ~/DD2360-HT25/rodinia_3.1/common/make.config
\end{verbatim}

\subsubsection{lavaMD}
For \verb|lavaMD|, the OpenMP Makefile was fine as is. For Cuda, the makefile had to change the \verb|-arch| flag value from \verb|sm_13| to \verb|sm_75|. 

\subsection{Input Problems and Execution Times}
Note that both these benchmarks came with their own timing prompt that was output as part of the program execution. Thus, no new timing blocks or code was added by us.
\subsubsection{hotspot3D}
We ran the OpenMP version with the following configuration (using the $512 \times 8$ input data):
\begin{verbatim}
./3D 512 8 100 ../../data/hotspot3D/power_512x8 ../../data/hotspot3D/temp_512x8 output.out
\end{verbatim}
With this, we got a total execution time of around \underline{14.511 seconds}. 

We ran the Cuda version with the exact same configuration. On doing so, we got a runtime of around \underline{0.028 seconds} for the kernel execution, and a runtime of around \underline{0.054 seconds} for the HtoD transfer + kernel execution + DtoH transfer. Note that the default benchmark code only timed the kernel execution, but we changed the timing around a little bit to get the data transfer times as well.

\subsubsection{lavaMD}
We ran the OpenMP version with the following configuration: \verb|./lavaMD -cores 8 -boxes1d 20|. With this, we got a total execution time of around \underline{1.982 seconds}. Note that the size of the input problem here is 20, as the \verb|-cores| argument is used to determine the number of threads launched and used by OpenMP.

We ran the Cuda version with the following configuration: \verb|./lavaMD -boxes1d 20|. With this, we got a total execution time of around \underline{0.093 seconds} for the HtoD transfer + kernel execution + DtoH transfer timing. For only kernel execution, we get a runtime of about \underline{0.084 seconds}. For the total runtime -- which includes a GPU initialization using (the now deprecated) \verb|cudaThreadSynchronize| and the mallocs and frees for memory, we get a total runtime of about \underline{0.287 seconds}. Note that here, we maintain the same input problem size as controlled by the \verb|-boxes1d| parameter. 

\subsection{Is there a Speedup in GPU?}
For \verb|hotspot3D|, there is an approx \underline{500x speedup} if we only consider kernel execution time, and an approx \underline{270x speedup} if we consider kernel execution and data transfer times in the CUDA execution compared to the OpenMP execution.
For \verb|lavaMD|, there is an approx \underline{24x speedup} if we only consider kernel execution time, an approx \underline{21x speedup} if we consider data transfers between host and device and kernel execution time, and an approx \underline{7x speedup} if we consider the total GPU execution time including GPU initialization and mallocs and frees.

Thus in all cases, not matter what execution time is considered for the GPU, there is a \underline{very noticeable speedup} when the same problem size is executed on the GPU as opposed to on the CPU. The main reason for this is likely the fact that GPUs have a lot more simple cores that are able to handle large computations with a lot more light-weight threads as opposed to the CPU, where the cores are a lot more powerful and complex.

Thus, while OpenMP offers us parallelism on the CPU, the threads are only truly able to utilize a few very powerful, specialized cores. On the other hand, GPU parallelism through CUDA is able to leverage possibly thousands of light-weight cores that specialize in arithmetic and are thus able to offer a much higher degree of parallelism, leading to faster runtimes. 

\todo{Double check this reasoning. Do we need to include any code for this region? Probably not since we did not write it?}

% content end
%###############################################################################

% \printbibliography

\end{document}
